# -*- coding: utf-8 -*-
"""
Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bk6AY7IR6GizE__2gb8Aggx8McCP62Zw
"""

# !pip install --upgrade numpy catboost
# !pip install --upgrade --force-reinstall numpy scipy pandas scikit-learn xgboost lightgbm catboost
# !pip install numpy==1.23.5
# !pip install --no-cache-dir catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, f1_score, classification_report, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

train_data = pd.read_csv('/content/train_data.csv')
test_data = pd.read_csv('/content/test_data.csv')

def load_and_explore_data(df, is_train=True):
    """Loads and explores the data from a csv file."""
    print("X----------------------------------------------------------------------X")
    print("Shape:\n", df.shape)
    print("Info:\n", df.info())
    print("Describe:\n", df.describe())
    print("Missing Values:\n", df.isnull().sum())
    if is_train:
      print("Bad Flag Distribution:\n", df['bad_flag'].value_counts(normalize=True))
    print("X----------------------------------------------------------------------X")

load_and_explore_data(train_data)
load_and_explore_data(test_data, is_train=False)

def preprocess_data(df, is_train=True):
    df = df.copy()
    if is_train:
        df = df.dropna(subset=['bad_flag'])

    # Separate features and target
    X = df.drop(['account_number', 'bad_flag'], axis=1, errors='ignore')
    y = df['bad_flag'] if 'bad_flag' in df else None

    # Identify and drop columns with all missing values
    missing_cols = X.columns[X.isnull().all()]
    X = X.drop(missing_cols, axis=1)
    print(f"Dropped columns with all missing values: {missing_cols}")

    # Handle missing values using median
    imputer = SimpleImputer(strategy='median')
    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

    # Scale numerical features
    scaler = StandardScaler()
    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

    return X, y

X_train, y_train = preprocess_data(train_data)
X_test, _ = preprocess_data(test_data, is_train=False)

# Models to Evaluate
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(verbose=0, random_state=42)
}

# Evaluate Models
results = []
roc_curves = {}

model_name, model_instance = list(models.items())[0]  # Extract model name and instance

logreg = model_instance
logreg.fit(X_train, y_train)
rf_preds = logreg.predict_proba(X_train)[:, 1]
rf_preds_class = logreg.predict(X_train)

# Compute Metrics
auc_score = roc_auc_score(y_train, rf_preds)
f1_class_0 = f1_score(y_train, rf_preds_class, pos_label=0)
f1_class_1 = f1_score(y_train, rf_preds_class, pos_label=1)
class_report = classification_report(y_train, rf_preds_class)

# Append to results
results.append({
    "Model": model_name,
    "AUC Score": auc_score,
    "F1 Score (Class 0)": f1_class_0,
    "F1 Score (Class 1)": f1_class_1,
    "Classification Report": class_report
})

# Calculate ROC Curve
fpr, tpr, thresholds = roc_curve(y_train, rf_preds)
roc_curves[model_name] = (fpr, tpr)

print(f"{model_name} AUC: {roc_auc_score(y_train, rf_preds)}")
print(f"{model_name} F1 Score Class 0:", f1_score(y_train, rf_preds_class, pos_label=0))
print(f"{model_name} F1 Score Class 1:", f1_score(y_train, rf_preds_class, pos_label=1))
print(classification_report(y_train, rf_preds_class))

model_name, model_instance = list(models.items())[1]  # Extract model name and instance

rf_model = model_instance
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict_proba(X_train)[:, 1]
rf_preds_class = rf_model.predict(X_train)

# Compute Metrics
auc_score = roc_auc_score(y_train, rf_preds)
f1_class_0 = f1_score(y_train, rf_preds_class, pos_label=0)
f1_class_1 = f1_score(y_train, rf_preds_class, pos_label=1)
class_report = classification_report(y_train, rf_preds_class)

# Append to results
results.append({
    "Model": model_name,
    "AUC Score": auc_score,
    "F1 Score (Class 0)": f1_class_0,
    "F1 Score (Class 1)": f1_class_1,
    "Classification Report": class_report
})

# Calculate ROC Curve
fpr, tpr, thresholds = roc_curve(y_train, rf_preds)
roc_curves[model_name] = (fpr, tpr)

print(f"{model_name} AUC: {roc_auc_score(y_train, rf_preds)}")
print(f"{model_name} F1 Score Class 0:", f1_score(y_train, rf_preds_class, pos_label=0))
print(f"{model_name} F1 Score Class 1:", f1_score(y_train, rf_preds_class, pos_label=1))
print(classification_report(y_train, rf_preds_class))

model_name, model_instance = list(models.items())[2]  # Extract model name and instance

xgb_model = model_instance
xgb_model.fit(X_train, y_train)
rf_preds = xgb_model.predict_proba(X_train)[:, 1]
rf_preds_class = xgb_model.predict(X_train)

# Compute Metrics
auc_score = roc_auc_score(y_train, rf_preds)
f1_class_0 = f1_score(y_train, rf_preds_class, pos_label=0)
f1_class_1 = f1_score(y_train, rf_preds_class, pos_label=1)
class_report = classification_report(y_train, rf_preds_class)

# Append to results
results.append({
    "Model": model_name,
    "AUC Score": auc_score,
    "F1 Score (Class 0)": f1_class_0,
    "F1 Score (Class 1)": f1_class_1,
    "Classification Report": class_report
})

# Calculate ROC Curve
fpr, tpr, thresholds = roc_curve(y_train, rf_preds)
roc_curves[model_name] = (fpr, tpr)

print(f"{model_name} AUC: {roc_auc_score(y_train, rf_preds)}")
print(f"{model_name} F1 Score Class 0:", f1_score(y_train, rf_preds_class, pos_label=0))
print(f"{model_name} F1 Score Class 1:", f1_score(y_train, rf_preds_class, pos_label=1))
print(classification_report(y_train, rf_preds_class))

model_name, model_instance = list(models.items())[3]  # Extract model name and instance

lgbm_model = model_instance
lgbm_model.fit(X_train, y_train)
rf_preds = lgbm_model.predict_proba(X_train)[:, 1]
rf_preds_class = lgbm_model.predict(X_train)

# Compute Metrics
auc_score = roc_auc_score(y_train, rf_preds)
f1_class_0 = f1_score(y_train, rf_preds_class, pos_label=0)
f1_class_1 = f1_score(y_train, rf_preds_class, pos_label=1)
class_report = classification_report(y_train, rf_preds_class)

# Append to results
results.append({
    "Model": model_name,
    "AUC Score": auc_score,
    "F1 Score (Class 0)": f1_class_0,
    "F1 Score (Class 1)": f1_class_1,
    "Classification Report": class_report
})

# Calculate ROC Curve
fpr, tpr, thresholds = roc_curve(y_train, rf_preds)
roc_curves[model_name] = (fpr, tpr)

print(f"{model_name} AUC: {roc_auc_score(y_train, rf_preds)}")
print(f"{model_name} F1 Score Class 0:", f1_score(y_train, rf_preds_class, pos_label=0))
print(f"{model_name} F1 Score Class 1:", f1_score(y_train, rf_preds_class, pos_label=1))
print(classification_report(y_train, rf_preds_class))

model_name, model_instance = list(models.items())[4]  # Extract model name and instance

cat_model = model_instance
cat_model.fit(X_train, y_train)
rf_preds = cat_model.predict_proba(X_train)[:, 1]
rf_preds_class = cat_model.predict(X_train)

# Compute Metrics
auc_score = roc_auc_score(y_train, rf_preds)
f1_class_0 = f1_score(y_train, rf_preds_class, pos_label=0)
f1_class_1 = f1_score(y_train, rf_preds_class, pos_label=1)
class_report = classification_report(y_train, rf_preds_class)

# Append to results
results.append({
    "Model": model_name,
    "AUC Score": auc_score,
    "F1 Score (Class 0)": f1_class_0,
    "F1 Score (Class 1)": f1_class_1,
    "Classification Report": class_report
})

# Calculate ROC Curve
fpr, tpr, thresholds = roc_curve(y_train, rf_preds)
roc_curves[model_name] = (fpr, tpr)

print(f"{model_name} AUC: {roc_auc_score(y_train, rf_preds)}")
print(f"{model_name} F1 Score Class 0:", f1_score(y_train, rf_preds_class, pos_label=0))
print(f"{model_name} F1 Score Class 1:", f1_score(y_train, rf_preds_class, pos_label=1))
print(classification_report(y_train, rf_preds_class))

# Find the best model based on AUC Score
best_model = max(results, key=lambda x: x["AUC Score"])

# Print the best model details
print(f"Best Model: {best_model['Model']}")
print(f"AUC Score: {best_model['AUC Score']}")
print(f"F1 Score (Class 0): {best_model['F1 Score (Class 0)']}")
print(f"F1 Score (Class 1): {best_model['F1 Score (Class 1)']}")
print("Classification Report:")
print(best_model["Classification Report"])

# Predict on Test Data
print("Generating Predictions...")
model = models[best_model['Model']] #Get the best model.
test_data['predicted_probability'] = model.predict_proba(X_test)[:, 1]

# Create Output File
submission_df = test_data[['account_number', 'predicted_probability']]
submission_df.to_csv('submission.csv', index=False)
print("Results saved to submission.csv")

# Plot ROC curve for the best model
fpr_best, tpr_best = roc_curves[best_model['Model']]
plt.figure(figsize=(8, 6))
plt.plot(fpr_best, tpr_best, label=f'{best_model["Model"]} (AUC = {best_model["AUC Score"]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for the Best Model')
plt.legend(loc='lower right')
plt.show()

# Plot ROC curves for all models
plt.figure(figsize=(10, 8))
for model_name, (fpr, tpr) in roc_curves.items():
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc_score(y_train, models[model_name].predict_proba(X_train)[:, 1]):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Models')
plt.legend(loc='lower right')
plt.show()

# F1 Score Bar Plot
model_names = [r['Model'] for r in results]
f1_class_0_scores = [r['F1 Score (Class 0)'] for r in results]
f1_class_1_scores = [r['F1 Score (Class 1)'] for r in results]

x = np.arange(len(model_names))
width = 0.35

plt.figure(figsize=(12, 6))
plt.bar(x - width/2, f1_class_0_scores, width, label='F1 Score (Class 0)')
plt.bar(x + width/2, f1_class_1_scores, width, label='F1 Score (Class 1)')

plt.xticks(x, model_names, rotation=45, ha='right')
plt.ylabel('F1 Score')
plt.title('F1 Scores for Each Model')
plt.legend()
plt.tight_layout()
plt.show()

# Feature Importance
final_model = models[best_model['Model']]  # Get the best model instance

if hasattr(final_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': final_model.feature_importances_})
    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

    # Plot Feature Importance
    plt.figure(figsize=(12, 8))
    plt.barh(feature_importance['Feature'][:20], feature_importance['Importance'][:20])
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.title('Top 20 Feature Importance')
    plt.gca().invert_yaxis()
    plt.show()

    # Print Top Features
    print("Top 10 Features:")
    print(feature_importance.head(10))
else:
    print(f"Feature importance is not available for {best_model['Model']}")

sns.countplot(x=train_data['bad_flag'])
plt.title("Bad Flag Distribution")
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=train_data.drop(columns=['bad_flag']))
plt.xticks(rotation=90)
plt.title("Feature Outliers")
plt.show()

plt.figure(figsize=(12, 6))
correlation = train_data.corr()['bad_flag'].drop('bad_flag')
correlation.sort_values().plot(kind='bar', colormap='coolwarm')
plt.title("Feature Correlation with Target")
plt.show()